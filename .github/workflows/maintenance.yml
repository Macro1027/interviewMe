name: AI Interview Platform Maintenance

on:
  schedule:
    - cron: '0 0 * * 1'  # Run every Monday at midnight
  workflow_dispatch:  # Allow manual triggering

jobs:
  update-dependencies:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      - name: Check for Python dependency updates
        id: pip-update
        run: |
          pip install pip-upgrader
          # Create a branch for dependency updates
          BRANCH_NAME="automated-dependency-update-$(date +'%Y-%m-%d')"
          git checkout -b $BRANCH_NAME
          
          # Run pip-upgrader to update dependencies
          pip-upgrader --skip-package-installation --skip-virtual-env --use-default --verbose requirements.txt
          
          echo "BRANCH_NAME=$BRANCH_NAME" >> $GITHUB_OUTPUT
          echo "CHANGES_MADE=$(git diff --name-only | grep requirements.txt | wc -l)" >> $GITHUB_OUTPUT
      
      - name: Set up Node.js for frontend
        if: hashFiles('frontend/package.json') != '' && steps.pip-update.outputs.CHANGES_MADE != '0'
        uses: actions/setup-node@v3
        with:
          node-version: '18'
      
      - name: Check for Node.js dependency updates
        if: hashFiles('frontend/package.json') != '' && steps.pip-update.outputs.CHANGES_MADE != '0'
        run: |
          cd frontend
          npm install -g npm-check-updates
          ncu -u
          npm install
          git add package.json package-lock.json
      
      - name: Create Pull Request if changes found
        if: steps.pip-update.outputs.CHANGES_MADE != '0'
        uses: peter-evans/create-pull-request@v5
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          commit-message: 'chore: update dependencies'
          title: 'Automated dependency updates'
          body: |
            This PR updates dependencies to their latest versions.
            
            Please review carefully and run the test suite before merging.
            
            Generated by the Maintenance workflow.
          branch: ${{ steps.pip-update.outputs.BRANCH_NAME }}
          base: develop
          labels: dependencies, automated-pr
  
  stale-issues:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/stale@v8
        with:
          repo-token: ${{ secrets.GITHUB_TOKEN }}
          stale-issue-message: 'This issue has been marked as stale due to 30 days of inactivity. It will be closed in 7 days if no further activity occurs.'
          stale-pr-message: 'This PR has been marked as stale due to 30 days of inactivity. It will be closed in 7 days if no further activity occurs.'
          stale-issue-label: 'stale'
          stale-pr-label: 'stale'
          days-before-stale: 30
          days-before-close: 7
          exempt-issue-labels: 'bug,security,enhancement,documentation,good first issue'
          exempt-pr-labels: 'waiting for review,work in progress,security'
  
  code-quality-scan:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
        with:
          fetch-depth: 0  # Fetch all history for quality analysis
      
      - name: SonarCloud Scan
        uses: SonarSource/sonarcloud-github-action@master
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
        with:
          args: >
            -Dsonar.projectKey=ai-interview-platform
            -Dsonar.organization=interview-ai
  
  performance-benchmarks:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest-benchmark
      
      - name: Run benchmarks
        run: |
          python -m pytest src/tests/benchmarks --benchmark-json=benchmark-results.json
      
      - name: Store benchmark results
        uses: benchmark-action/github-action-benchmark@v1
        with:
          tool: 'pytest'
          output-file-path: benchmark-results.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          comment-on-alert: true
          alert-threshold: '150%'
          fail-on-alert: false 