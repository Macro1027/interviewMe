# Hugging Face API Configuration

# API configuration
api:
  base_url: "https://api-inference.huggingface.co/models"
  timeout: 60  # seconds (longer timeout due to model loading)

# Default model settings
default_model: "mistralai/Mixtral-8x7B-Instruct-v0.1"
available_models:
  - name: "mistralai/Mixtral-8x7B-Instruct-v0.1"
    description: "Mixtral MoE model fine-tuned for instructions"
    max_tokens: 4096
    context_window: 8192
  - name: "meta-llama/Llama-2-13b-chat-hf"
    description: "Meta's LLaMa 2 chat model (13B)"
    max_tokens: 4096
    context_window: 4096
  - name: "codellama/CodeLlama-13b-Instruct-hf"
    description: "CodeLlama model for code generation (13B)"
    max_tokens: 2048
    context_window: 4096
  - name: "google/flan-t5-xl"
    description: "Google's Flan-T5 model for instruction following"
    max_tokens: 1024
    context_window: 2048

# Default generation parameters
parameters:
  temperature: 0.7
  top_p: 0.95
  top_k: 50
  repetition_penalty: 1.2
  max_new_tokens: 1024
  return_full_text: false

# Token formatting
token_format:
  system_prefix: "<|system|>\n"
  system_suffix: "\n"
  user_prefix: "<|user|>\n"
  user_suffix: "\n"
  assistant_prefix: "<|assistant|>\n"
  assistant_suffix: "\n"

# Fallback settings
fallback:
  enabled: true
  retry_attempts: 2
  retry_delay: 3.0  # seconds, with exponential backoff
  timeout_multiplier: 1.5 